
export default {
  async fetch(request, env, ctx) {
    const url = new URL(request.url);
    if (request.method === "OPTIONS") return new Response(null, { headers: cors() });

    try {
      // API routes
      if (url.pathname === "/api/harvest" && request.method === "POST") return await handleHarvest(request, env);
      if (url.pathname === "/api/ris" && request.method === "POST") return await handleRIS(request);
      if (url.pathname === "/api/health") return await health(env);
      if (url.pathname === "/api/harvest-now") return await harvestNow(env);
      if (url.pathname === "/api/force-harvest") return await forceHarvest(env);
      if (url.pathname === "/api/test-elis") return await testEliS(env);
      if (url.pathname === "/api/elis-live-search" && request.method === "POST") return await handleEliSLiveSearch(request, env);
      if (url.pathname === "/api/harvest-incremental" && request.method === "POST") return await handleIncrementalHarvest(request, env);

      return json({ 
        ok: true, 
        service: "Academic Library Harvester", 
        version: "2.0.0",
        timestamp: new Date().toISOString()
      });
    } catch (err) {
      await logError(env, "Global", err);
      return json({ success: false, error: err.message }, 500);
    }
  },

  async scheduled(event, env, ctx) {
    try {
      if (event.cron === "0 2 * * *") { // Daily at 2 AM
        console.log("ğŸ”„ Starting automated daily harvest (100 records per repository)");
        await dailyHarvest(env);
        await updateMeta(env);
        console.log("âœ… Automated harvest completed");
      }
    } catch (err) {
      await logError(env, "Scheduled", err);
    }
  }
};

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

const USER_AGENT = "Academic-Library-Harvester/2.0.0";
const PAGE_SIZE_DEFAULT = 24;
const RECORDS_PER_REPOSITORY = 100;
const INCREMENTAL_RECORDS = 20;

// DSpace endpoints including E-LIS - FIXED URLs
const DSPACE_ENDPOINTS = {
  uct:   "https://open.uct.ac.za/oai/request",
  spu:   "https://openhub.spu.ac.za/oai/request",
  sun:   "https://scholar.sun.ac.za/server/oai/request",
  ufs:   "https://scholar.ufs.ac.za/server/oai/request",
  up:    "https://repository.up.ac.za/server/oai/request",
  unisa: "https://uir.unisa.ac.za/server/oai/request",
  nwu:   "https://repository.nwu.ac.za/server/oai/request",
  wits:  "https://wiredspace.wits.ac.za/server/oai/request",
  cut:   "https://cutscholar.cut.ac.za/server/oai/request",
  elis:  "https://eprints.rclis.org/cgi/oai2" // Changed to https
};

const RESEARCH_DATA_SOURCES = [
  {
    name: "dryad",
    apiUrl: "https://datadryad.org/api/v2/search",
    displayName: "Dryad Digital Repository",
    params: "&page=1&per_page=100"
  },
  {
    name: "mendeley", 
    apiUrl: "https://api.datacite.org/dois",
    displayName: "Mendeley Data",
    params: "&client-id=mendeley&page[size]=100"
  },
  {
    name: "zenodo",
    apiUrl: "https://zenodo.org/api/records",
    displayName: "Zenodo",
    params: "&size=100&sort=mostrecent"
  }
];

const KV = {
  theses:   "harvest_theses",
  articles: "harvest_articles", 
  research: "harvest_research",
  meta:     "harvest_meta",
  lastErr:  "last_error",
  lastHarvest: "last_harvest"
};

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Core Routes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

async function handleHarvest(request, env) {
  const body = await request.json().catch(() => ({}));
  const {
    category = "all",
    query = "",
    page = 1,
    pageSize = PAGE_SIZE_DEFAULT,
    filters = {}
  } = body;

  try {
    const base = await loadBase(env, category);
    let filtered = applyFacetFilters(base, filters, query);

    const facets = buildFacets(filtered);
    const { slice, total } = paginate(filtered, Number(page), Number(pageSize));

    return json({
      success: true,
      results: slice,
      total,
      page: Number(page),
      pageSize: Number(pageSize),
      hasMore: Number(page) * Number(pageSize) < total,
      facets
    });
  } catch (err) {
    await logError(env, "Harvest", err);
    return json({ success: false, error: err.message }, 500);
  }
}

async function handleIncrementalHarvest(request, env) {
  const body = await request.json().catch(() => ({}));
  const { category = "all" } = body;

  try {
    console.log(`ğŸ”„ Incremental harvest triggered for category: ${category}`);
    
    let newRecords = 0;
    
    if (category === "all" || category === "theses" || category === "articles") {
      console.log("ğŸ“š Incremental harvesting from DSpace repositories...");
      const dspaceResults = await harvestDSpaceRepositoriesIncremental(env, INCREMENTAL_RECORDS);
      newRecords += dspaceResults;
    }
    
    if (category === "all" || category === "research") {
      console.log("ğŸ”¬ Incremental harvesting from research data sources...");
      const researchResults = await harvestResearchDataIncremental(env, INCREMENTAL_RECORDS);
      newRecords += researchResults;
    }
    
    await updateMeta(env);
    
    return json({
      success: true,
      message: `Incremental harvest completed`,
      newRecords: newRecords,
      category: category,
      timestamp: new Date().toISOString()
    });
    
  } catch (err) {
    await logError(env, "IncrementalHarvest", err);
    return json({ success: false, error: err.message }, 500);
  }
}

async function handleEliSLiveSearch(request, env) {
  const body = await request.json().catch(() => ({}));
  const { query = "", page = 1, pageSize = PAGE_SIZE_DEFAULT } = body;

  try {
    console.log(`ğŸ” E-LIS Live Search: "${query}"`);
    const results = await searchEliSDirectly(query, pageSize);
    
    return json({
      success: true,
      results: results,
      total: results.length,
      page: Number(page),
      pageSize: Number(pageSize),
      hasMore: false,
      source: "E-LIS Live Search"
    });
  } catch (err) {
    await logError(env, "EliSLiveSearch", err);
    return json({ success: false, error: err.message }, 500);
  }
}

async function searchEliSDirectly(query = "", limit = 100) {
  const results = [];
  
  try {
    console.log(`   Searching E-LIS for: "${query}", limit: ${limit}`);
    let token = null;
    let pages = 0;
    const maxPages = 10; // Increased to get more records
    
    while (results.length < limit && pages < maxPages) {
      const searchUrl = token 
        ? `https://eprints.rclis.org/cgi/oai2?verb=ListRecords&resumptionToken=${encodeURIComponent(token)}`
        : `https://eprints.rclis.org/cgi/oai2?verb=ListRecords&metadataPrefix=oai_dc`;
      
      console.log(`   Fetching E-LIS page ${pages + 1}, token: ${token ? 'yes' : 'no'}`);
      
      const xml = await fetchWithRetry(searchUrl);
      if (!xml || xml.length < 100) break;

      if (xml.includes('<error code=')) {
        const errorMatch = xml.match(/<error code="([^"]*)">([^<]*)<\/error>/);
        if (errorMatch) {
          console.log(`   E-LIS error: ${errorMatch[1]} - ${errorMatch[2]}`);
          if (errorMatch[1] === 'noRecordsMatch') break;
        }
      }

      const { records, next } = parseOai(xml, "elis");
      console.log(`   E-LIS page ${pages + 1} found ${records.length} records`);
      
      // Filter by query if provided
      let filteredRecords = records;
      if (query) {
        const q = query.toLowerCase();
        filteredRecords = records.filter(record => {
          const searchText = `${record.title || ""} ${record.description || ""} ${(record.authors || []).join(" ")} ${(record.keywords || []).join(" ")}`.toLowerCase();
          return searchText.includes(q);
        });
        console.log(`   After query filtering: ${filteredRecords.length} records`);
      }
      
      results.push(...filteredRecords);
      token = next;
      pages++;
      
      if (!token) break;
      
      // Break if we have enough records
      if (results.length >= limit) {
        results.length = limit; // Trim to limit
        break;
      }
      
      await sleep(1000);
    }
    
    console.log(`   E-LIS search completed: ${results.length} records total`);
    return results;
    
  } catch (err) {
    console.error("   E-LIS direct search error:", err.message);
    return results;
  }
}

async function handleRIS(request) {
  const { records = [] } = await request.json().catch(() => ({ records: [] }));
  const lines = records.flatMap(toRIS);
  return new Response(lines.join("\r\n") + "\r\n", {
    headers: {
      "Content-Type": "application/x-research-info-systems",
      "Content-Disposition": 'attachment; filename="library_export.ris"',
      ...cors()
    }
  });
}

async function health(env) {
  const [theses, articles, research, meta, lastHarvest] = await Promise.all([
    kvGetJSON(env, KV.theses, []),
    kvGetJSON(env, KV.articles, []),
    kvGetJSON(env, KV.research, []),
    kvGetJSON(env, KV.meta, {}),
    env.HARVEST_KV.get(KV.lastHarvest)
  ]);

  const totalRecords = theses.length + articles.length + research.length;

  return json({
    ok: true,
    service: "Academic Library Harvester",
    timestamp: new Date().toISOString(),
    repositories: {
      academic: Object.keys(DSPACE_ENDPOINTS).length,
      research_data: RESEARCH_DATA_SOURCES.length,
      includes_elis: true
    },
    data: {
      total_records: totalRecords,
      theses: theses.length,
      articles: articles.length,
      research: research.length
    },
    harvest: {
      last_harvest: lastHarvest || "Never",
      next_harvest: "Daily at 2 AM"
    }
  });
}

async function harvestNow(env) {
  try {
    console.log("ğŸš€ Manual harvest triggered (100 records per repository)");
    await dailyHarvest(env);
    await updateMeta(env);
    
    return json({ 
      success: true, 
      message: "Manual harvest completed - 100 records from each repository",
      timestamp: new Date().toISOString()
    });
  } catch (err) {
    await logError(env, "HarvestNow", err);
    return json({ success: false, error: err.message }, 500);
  }
}

async function forceHarvest(env) {
  try {
    console.log("ğŸ’¥ Force harvest triggered");
    
    await Promise.all([
      env.HARVEST_KV.delete(KV.theses),
      env.HARVEST_KV.delete(KV.articles),
      env.HARVEST_KV.delete(KV.research),
      env.HARVEST_KV.delete(KV.meta)
    ]);
    
    console.log("ğŸ§¹ Cache cleared, starting fresh harvest");
    await dailyHarvest(env);
    await updateMeta(env);
    
    return json({ 
      success: true, 
      message: "Force harvest completed - cache cleared and fresh data harvested",
      timestamp: new Date().toISOString()
    });
  } catch (err) {
    await logError(env, "ForceHarvest", err);
    return json({ success: false, error: err.message }, 500);
  }
}

async function testEliS(env) {
  try {
    console.log("ğŸ§ª Testing E-LIS connection");
    const testUrl = "https://eprints.rclis.org/cgi/oai2?verb=Identify";
    const response = await fetchWithRetry(testUrl);
    
    if (response && response.includes('E-LIS')) {
      return json({
        success: true,
        message: "E-LIS repository is accessible",
        repository: "E-LIS e-prints in Library and Information Science",
        status: "Online"
      });
    } else {
      return json({
        success: false,
        message: "E-LIS repository test failed"
      }, 500);
    }
  } catch (err) {
    return json({
      success: false,
      error: err.message,
      message: "E-LIS test failed with error"
    }, 500);
  }
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Daily Harvest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

async function dailyHarvest(env) {
  console.log("ğŸ”„ Starting daily harvest including E-LIS (100 records per repository)");
  const harvestStart = Date.now();
  
  try {
    await harvestDSpaceRepositories(env);
    await harvestResearchData(env);
    await env.HARVEST_KV.put(KV.lastHarvest, new Date().toISOString());
    
    const duration = Math.round((Date.now() - harvestStart) / 1000);
    console.log(`âœ… Daily harvest completed in ${duration} seconds`);
    
  } catch (err) {
    await logError(env, "DailyHarvest", err);
    throw err;
  }
}

async function harvestDSpaceRepositories(env) {
  const allTheses = [], allArticles = [];
  
  const [existingTheses, existingArticles] = await Promise.all([
    kvGetJSON(env, KV.theses, []),
    kvGetJSON(env, KV.articles, [])
  ]);

  console.log(`ğŸ“š Harvesting ${Object.keys(DSPACE_ENDPOINTS).length} repositories including E-LIS`);

  for (const [id, endpoint] of Object.entries(DSPACE_ENDPOINTS)) {
    try {
      console.log(`ğŸ” Testing ${id} at ${endpoint}`);
      
      const testUrl = `${endpoint}?verb=Identify`;
      const testResponse = await fetchWithRetry(testUrl);
      
      if (!testResponse) {
        console.log(`â­ï¸ Skipping ${id} - no response from endpoint`);
        continue;
      }
      
      if (testResponse.includes('<error code=')) {
        console.log(`â­ï¸ Skipping ${id} - endpoint returned error: ${testResponse}`);
        continue;
      }
      
      console.log(`âœ… ${id} is accessible, starting harvest...`);
      const { t: theses, a: articles } = await harvestDSpaceRepo(id, endpoint, RECORDS_PER_REPOSITORY);
      allTheses.push(...theses);
      allArticles.push(...articles);
      console.log(`âœ… ${id}: ${theses.length} theses, ${articles.length} articles`);
    } catch (err) {
      console.error(`âŒ ${id} failed:`, err.message);
      await logError(env, "DSpace:" + id, err);
    }
    
    await sleep(2000);
  }

  const mergedTheses = accumulativeDedupe([...existingTheses, ...allTheses]);
  const mergedArticles = accumulativeDedupe([...existingArticles, ...allArticles]);
  
  await kvPutJSON(env, KV.theses, mergedTheses);
  await kvPutJSON(env, KV.articles, mergedArticles);
  
  console.log(`ğŸ‰ Repositories: ${allTheses.length} new theses, ${allArticles.length} new articles`);
}

async function harvestDSpaceRepositoriesIncremental(env, limit = 20) {
  let totalNewRecords = 0;
  
  const [existingTheses, existingArticles] = await Promise.all([
    kvGetJSON(env, KV.theses, []),
    kvGetJSON(env, KV.articles, [])
  ]);

  console.log(`ğŸ“š Incremental harvesting from ${Object.keys(DSPACE_ENDPOINTS).length} repositories`);

  for (const [id, endpoint] of Object.entries(DSPACE_ENDPOINTS)) {
    try {
      console.log(`ğŸ” Testing ${id} for incremental harvest`);
      
      const testUrl = `${endpoint}?verb=Identify`;
      const testResponse = await fetchWithRetry(testUrl);
      
      if (!testResponse || testResponse.includes('<error code=')) {
        console.log(`â­ï¸ Skipping ${id} - endpoint not accessible`);
        continue;
      }
      
      console.log(`âœ… Incremental harvesting ${id}`);
      const { t: theses, a: articles } = await harvestDSpaceRepo(id, endpoint, limit);
      
      // Filter out duplicates before adding
      const newTheses = theses.filter(newThesis => 
        !existingTheses.some(existing => 
          createRecordSignature(existing) === createRecordSignature(newThesis)
        )
      );
      
      const newArticles = articles.filter(newArticle => 
        !existingArticles.some(existing => 
          createRecordSignature(existing) === createRecordSignature(newArticle)
        )
      );
      
      if (newTheses.length > 0) {
        await kvPutJSON(env, KV.theses, [...existingTheses, ...newTheses]);
        console.log(`âœ… ${id}: Added ${newTheses.length} new theses`);
        totalNewRecords += newTheses.length;
      }
      
      if (newArticles.length > 0) {
        await kvPutJSON(env, KV.articles, [...existingArticles, ...newArticles]);
        console.log(`âœ… ${id}: Added ${newArticles.length} new articles`);
        totalNewRecords += newArticles.length;
      }
      
    } catch (err) {
      console.error(`âŒ ${id} incremental harvest failed:`, err.message);
      await logError(env, "DSpaceIncremental:" + id, err);
    }
    
    await sleep(1000);
  }

  return totalNewRecords;
}

async function harvestResearchData(env) {
  const allResearch = [];
  const existingResearch = await kvGetJSON(env, KV.research, []);
  
  console.log(`ğŸ”¬ Harvesting research data from ${RESEARCH_DATA_SOURCES.length} sources`);

  for (const source of RESEARCH_DATA_SOURCES) {
    try {
      console.log(`   ${source.displayName}`);
      const recs = await fetchResearchSource(source, "", RECORDS_PER_REPOSITORY);
      allResearch.push(...recs);
      console.log(`   âœ… ${source.name}: ${recs.length} records`);
    } catch (err) {
      console.error(`   âŒ ${source.name} failed:`, err.message);
      await logError(env, "Research:" + source.name, err);
    }
    await sleep(2000);
  }
  
  const mergedResearch = accumulativeDedupe([...existingResearch, ...allResearch]);
  await kvPutJSON(env, KV.research, mergedResearch);
  
  console.log(`ğŸ‰ Research: ${allResearch.length} new records`);
}

async function harvestResearchDataIncremental(env, limit = 20) {
  let totalNewRecords = 0;
  const existingResearch = await kvGetJSON(env, KV.research, []);
  
  console.log(`ğŸ”¬ Incremental harvesting research data from ${RESEARCH_DATA_SOURCES.length} sources`);

  for (const source of RESEARCH_DATA_SOURCES) {
    try {
      console.log(`   ${source.displayName} incremental harvest`);
      const recs = await fetchResearchSource(source, "", limit);
      
      // Filter out duplicates
      const newRecs = recs.filter(newRec => 
        !existingResearch.some(existing => 
          createRecordSignature(existing) === createRecordSignature(newRec)
        )
      );
      
      if (newRecs.length > 0) {
        await kvPutJSON(env, KV.research, [...existingResearch, ...newRecs]);
        console.log(`   âœ… ${source.name}: Added ${newRecs.length} new records`);
        totalNewRecords += newRecs.length;
      }
    } catch (err) {
      console.error(`   âŒ ${source.name} incremental harvest failed:`, err.message);
      await logError(env, "ResearchIncremental:" + source.name, err);
    }
    await sleep(1000);
  }
  
  return totalNewRecords;
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Harvesting Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

async function harvestDSpaceRepo(id, endpoint, limit = 100) {
  const theses = [], articles = [];
  let token = null, pages = 0, maxPages = 10;

  console.log(`   Starting harvest from ${id}, limit: ${limit} records`);

  while (pages < maxPages && (theses.length + articles.length) < limit) {
    const url = token
      ? `${endpoint}?verb=ListRecords&resumptionToken=${encodeURIComponent(token)}`
      : `${endpoint}?verb=ListRecords&metadataPrefix=oai_dc`;

    console.log(`   Fetching from ${id}, page ${pages + 1}, token: ${token ? 'yes' : 'no'}`);
    
    const xml = await fetchWithRetry(url);
    if (!xml || xml.length < 100) {
      console.log(`   No data received from ${id}`);
      break;
    }

    if (xml.includes('<error code=')) {
      const errorMatch = xml.match(/<error code="([^"]*)">([^<]*)<\/error>/);
      if (errorMatch) {
        console.log(`   ${id} returned error: ${errorMatch[1]} - ${errorMatch[2]}`);
        if (errorMatch[1] === 'noRecordsMatch') break;
      }
    }

    const { records, next } = parseOai(xml, id);
    console.log(`   ${id} page ${pages + 1} parsed ${records.length} records`);
    
    for (const r of records) {
      if (/thesis|dissertation/i.test(r.type)) theses.push(r);
      else if (/article/i.test(r.type)) articles.push(r);
      else if (r.type && r.type !== "Other") articles.push(r);
      
      if ((theses.length + articles.length) >= limit) break;
    }

    token = next;
    pages++;
    if (!token) break;
    
    await sleep(1500);
  }

  console.log(`   ${id} harvest completed: ${theses.length} theses, ${articles.length} articles`);
  return { t: theses.slice(0, limit), a: articles.slice(0, limit) };
}

async function fetchResearchSource(source, query = "", limit = 100) {
  const out = [];
  const size = Math.min(100, limit);

  try {
    for (let page = 1; out.length < limit; page++) {
      let url = "";
      if (source.name === "dryad") {
        url = `${source.apiUrl}?q=${encodeURIComponent(query || "*")}&page=${page}&per_page=${size}`;
      } else if (source.name === "mendeley") {
        url = `${source.apiUrl}?query=${encodeURIComponent(query || "*")}${source.params}&page[number]=${page}`;
      } else if (source.name === "zenodo") {
        url = `${source.apiUrl}?q=${encodeURIComponent(query || "*")}${source.params}&page=${page}`;
      }

      console.log(`   Fetching from ${source.name}, page ${page}: ${url}`);
      
      const data = await fetchJSON(url);
      if (!data) break;

      if (source.name === "dryad") {
        const results = data?._embedded?.["stash:items"] || data?.results || [];
        if (!results.length) break;
        for (const it of results) {
          if (out.length >= limit) break;
          const a = it.attributes || {};
          const doi = a.doi || "";
          out.push({
            id: `dryad-${it.id}`,
            title: a.title || "Untitled",
            authors: (a.authors || []).map(x => x.fullName || x).filter(Boolean),
            description: a.abstract || "",
            keywords: a.keywords || [],
            year: year(a.publicationDate),
            source: source.displayName,
            type: "Research Data",
            identifier: doi || String(it.id),
            identifierType: doi ? "DOI" : "Dryad ID",
            url: doi ? `https://doi.org/${doi}` : `https://datadryad.org/stash/dataset/${it.id}`
          });
        }
      } else if (source.name === "mendeley" && Array.isArray(data?.data)) {
        for (const it of data.data) {
          if (out.length >= limit) break;
          const at = it.attributes || {};
          const doi = at.doi || "";
          out.push({
            id: `mendeley-${it.id}`,
            title: at.title || "Untitled",
            authors: (at.creators || []).map(c => c?.name).filter(Boolean),
            description: stripHtml(at.descriptions?.[0]?.description || ""),
            keywords: (at.subjects || []).map(s => s.subject).filter(Boolean),
            year: year(at.published),
            source: source.displayName,
            type: "Research Data",
            identifier: doi || String(it.id),
            identifierType: doi ? "DOI" : "ID",
            url: at.url || (doi ? `https://doi.org/${doi}` : "")
          });
        }
        if (data?.meta?.totalPages && page >= data.meta.totalPages) break;
      } else if (source.name === "zenodo" && Array.isArray(data?.hits?.hits)) {
        for (const it of data.hits.hits) {
          if (out.length >= limit) break;
          const md = it.metadata || {};
          const doi = md.doi || "";
          out.push({
            id: `zenodo-${it.id}`,
            title: md.title || "Untitled",
            authors: (md.creators || []).map(c => c.name).filter(Boolean),
            description: md.description || "",
            keywords: md.keywords || [],
            year: year(md.publication_date),
            source: source.displayName,
            type: "Research Data",
            identifier: doi || String(it.id),
            identifierType: doi ? "DOI" : "Zenodo ID",
            url: md.doi ? `https://doi.org/${md.doi}` : it.links?.html || ""
          });
        }
        if (data.hits.hits.length < size) break;
      } else {
        break;
      }
    }
  } catch (err) {
    console.error(`   Error fetching from ${source.name}:`, err.message);
  }

  console.log(`   ${source.name} returned ${out.length} records`);
  return out;
}

/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Utility Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

function cors() {
  return { 
    "Access-Control-Allow-Origin": "*", 
    "Access-Control-Allow-Methods": "GET, POST, OPTIONS", 
    "Access-Control-Allow-Headers": "Content-Type, Authorization" 
  };
}

function json(obj, status=200) { 
  return new Response(JSON.stringify(obj, null, 2), { 
    status, 
    headers: { "Content-Type": "application/json", ...cors() } 
  }); 
}

async function kvGetJSON(env, key, fallback) {
  try { 
    const v = await env.HARVEST_KV.get(key); 
    return v ? JSON.parse(v) : fallback; 
  } catch (e) { 
    console.error(`KV get error for ${key}:`, e.message);
    return fallback; 
  }
}

async function kvPutJSON(env, key, value) {
  try { 
    await env.HARVEST_KV.put(key, JSON.stringify(value)); 
  } catch (e) { 
    console.error("KV put fail", key, e.message); 
    throw e; 
  }
}

async function updateMeta(env) {
  const [t, a, r] = await Promise.all([
    kvGetJSON(env, KV.theses, []),
    kvGetJSON(env, KV.articles, []),
    kvGetJSON(env, KV.research, [])
  ]);
  await env.HARVEST_KV.put(KV.meta, JSON.stringify({
    lastUpdated: new Date().toISOString(),
    counts: { theses: t.length, articles: a.length, research: r.length, total: t.length + a.length + r.length }
  }));
}

async function loadBase(env, category) {
  if (category === "theses")   return kvGetJSON(env, KV.theses, []);
  if (category === "articles") return kvGetJSON(env, KV.articles, []);
  if (category === "research") return kvGetJSON(env, KV.research, []);
  const [t, a, r] = await Promise.all([
    kvGetJSON(env, KV.theses, []),
    kvGetJSON(env, KV.articles, []),
    kvGetJSON(env, KV.research, [])
  ]);
  return accumulativeDedupe([...t, ...a, ...r]);
}

function accumulativeDedupe(arr) {
  const seen = new Set();
  const unique = [];
  
  for (const r of arr) {
    if (!r.id) continue;
    
    const signature = createRecordSignature(r);
    
    if (!seen.has(signature)) {
      seen.add(signature);
      unique.push(r);
    }
  }
  
  return unique;
}

function createRecordSignature(record) {
  const title = (record.title || '').trim().toLowerCase();
  const authors = Array.isArray(record.authors) 
    ? record.authors.join(',').toLowerCase() 
    : (record.authors || '').toLowerCase();
  const source = (record.source || '').toLowerCase();
  const year = record.year || '';
  const identifier = record.identifier || '';
  
  if (identifier) {
    return `${source}-${identifier}`.toLowerCase();
  }
  
  return `${source}-${title}-${authors}-${year}`.toLowerCase();
}

function applyFacetFilters(items, f = {}, keyword = "") {
  const yearF   = f.year?.toString().trim();
  const repoF   = f.repository?.trim();
  const typeF   = f.type?.trim();
  const authorF = f.author?.trim().toLowerCase();

  let out = (items || []).filter(r =>
    (!yearF   || String(r.year) === yearF) &&
    (!repoF   || r.source === repoF) &&
    (!typeF   || r.type === typeF) &&
    (!authorF || (r.authors || []).some(a => a?.toLowerCase().includes(authorF)))
  );

  if (keyword) {
    const q = keyword.toLowerCase();
    out = out.filter(r => {
      const hay = `${r.title||""} ${r.description||""} ${(r.keywords||[]).join(" ")} ${(r.authors||[]).join(" ")}`.toLowerCase();
      return hay.includes(q);
    });
  }

  return out;
}

function buildFacets(records) {
  const years = new Map(), authors = new Map(), repos = new Map(), types = new Map();
  for (const r of records || []) {
    if (r.year) years.set(r.year, (years.get(r.year)||0)+1);
    (r.authors||[]).forEach(a => a && authors.set(a, (authors.get(a)||0)+1));
    if (r.source) repos.set(r.source, (repos.get(r.source)||0)+1);
    if (r.type)   types.set(r.type,   (types.get(r.type)||0)+1);
  }
  const mapify = (m) => [...m.entries()].sort((a,b)=>b[1]-a[1]).map(([name,count])=>({ name, count }));
  return {
    years: mapify(years),
    authors: mapify(authors).slice(0,100),
    repositories: mapify(repos),
    types: mapify(types)
  };
}

function paginate(array, page, pageSize) {
  const start = (page - 1) * pageSize;
  const end = start + pageSize;
  const slice = array.slice(start, end);
  return { slice, total: array.length };
}

async function fetchWithRetry(url, retries = 3) {
  for (let i = 0; i < retries; i++) {
    try {
      console.log(`   Fetch attempt ${i + 1}: ${url}`);
      const response = await fetch(url, {
        headers: {
          "Accept": "application/xml, text/xml, */*;q=0.1",
          "User-Agent": USER_AGENT
        }
      });
      
      if (response.ok) {
        const text = await response.text();
        console.log(`   Fetch successful, length: ${text.length}`);
        return text;
      } else {
        console.log(`   Fetch failed with status: ${response.status}`);
      }
    } catch (err) {
      console.warn(`   Attempt ${i + 1} failed:`, err.message);
    }
    
    if (i < retries - 1) {
      await sleep(1000 * (i + 1));
    }
  }
  console.log(`   All fetch attempts failed for: ${url}`);
  return null;
}

async function fetchJSON(url) {
  try {
    console.log(`   Fetching JSON: ${url}`);
    const r = await fetch(url, { 
      headers: { 
        "Accept":"application/json", 
        "User-Agent": USER_AGENT 
      } 
    });
    if (r.ok) {
      const data = await r.json();
      console.log(`   JSON fetch successful`);
      return data;
    } else {
      console.log(`   JSON fetch failed: ${r.status}`);
      return null;
    }
  } catch (e) {
    console.error(`   fetchJSON fail for ${url}:`, e.message);
    return null;
  }
}

function parseOai(xml, id) {
  if (!xml || xml.length < 100) {
    console.log(`   parseOai: No XML to parse for ${id}`);
    return { records: [], next: null };
  }

  let next = xml.match(/<resumptionToken[^>]*>([\s\S]*?)<\/resumptionToken>/i)?.[1]?.trim() || null;
  const recs = [];

  console.log(`   parseOai for ${id}, resumptionToken: ${next ? 'yes' : 'no'}`);

  const hasOAI = /<OAI-PMH/i.test(xml) || /<record>/i.test(xml);
  if (hasOAI) {
    const recRegex = /<record>[\s\S]*?<\/record>/gi;
    let m;
    let count = 0;
    while ((m = recRegex.exec(xml))) {
      const block = m[0];
      const record = makeRecordFromBlock(block, id);
      if (record && record.title && record.title !== "Untitled") {
        recs.push(record);
        count++;
      }
    }
    console.log(`   parseOai found ${count} records using record regex`);
    return { records: recs, next };
  }

  const altRegex = /<oai_dc:dc[\s\S]*?<\/oai_dc:dc>/gi;
  const blocks = xml.match(altRegex) || [];
  console.log(`   parseOai found ${blocks.length} blocks using alt regex`);
  for (const b of blocks) {
    const record = makeRecordFromBlock(b, id);
    if (record && record.title && record.title !== "Untitled") recs.push(record);
  }
  
  next = null;
  return { records: recs, next };
}

function makeRecordFromBlock(block, id) {
  const title = pick(extract(block, "dc:title")) || "Untitled";
  const creators = extract(block, "dc:creator");
  const desc = pick(extract(block, "dc:description")) || "";
  const subjects = extract(block, "dc:subject");
  const types = extract(block, "dc:type").join(" ").toLowerCase();
  const date = pick(extract(block, "dc:date"));
  const identifiers = extract(block, "dc:identifier");

  const { identifier, identifierType, url } = bestIdForDSpace(identifiers, id);

  return {
    id: `${id}-${rid()}`,
    title,
    authors: creators,
    description: desc,
    keywords: subjects,
    year: year(date),
    source: sourceMap(id),
    type: /(thesis|dissertation)/i.test(types) ? "Thesis/Dissertation" :
          /article/i.test(types) ? "Journal Article" : "Other",
    identifier, 
    identifierType, 
    url: url || "#"
  };
}

function bestIdForDSpace(ids = [], repositoryId) {
  if (!ids || !ids.length) {
    return { identifier: "", identifierType: "", url: "" };
  }

  // Priority 1: Handle URLs
  const handleUri = ids.find(id => 
    id && typeof id === 'string' && 
    (id.includes('/handle/') || id.includes('hdl.handle.net'))
  );
  
  if (handleUri) {
    return {
      identifier: handleUri,
      identifierType: "Handle",
      url: handleUri
    };
  }

  // Priority 2: DOIs
  const doi = ids.find(id => /^10\.\d{4,9}\/\S+/i.test(id));
  if (doi) {
    return {
      identifier: doi,
      identifierType: "DOI",
      url: `https://doi.org/${doi}`
    };
  }

  // Priority 3: Any HTTP URL
  const httpUrl = ids.find(id => id && typeof id === 'string' && id.startsWith('http'));
  if (httpUrl) {
    return {
      identifier: httpUrl,
      identifierType: "URL",
      url: httpUrl
    };
  }

  // Fallback
  const fallback = ids[0] || "";
  return {
    identifier: fallback,
    identifierType: "Identifier",
    url: fallback.startsWith('http') ? fallback : "#"
  };
}

function toRIS(r) {
  const isThesis = /(thesis|dissertation)/i.test(r.type||"");
  const isArticle = /article/i.test(r.type||"");
  const out = [];
  out.push("TY  - " + (isThesis ? "THES" : isArticle ? "JOUR" : "DATA"));
  if (r.title) out.push("TI  - " + r.title);
  (r.authors||[]).forEach(a => a && out.push("AU  - " + a));
  if (r.year) out.push("PY  - " + r.year);
  if (r.source) out.push("PB  - " + r.source);
  if (r.description) out.push("AB  - " + r.description.slice(0, 500));
  if (r.url) out.push("UR  - " + r.url);
  if (/^10\.\d{4,9}\//.test(r.identifier||"")) out.push("DO  - " + r.identifier);
  out.push("ER  - ");
  return out;
}

function sourceMap(id) {
  const map = {
    uct: "University of Cape Town",
    spu: "Sol Plaatje University",
    sun: "Stellenbosch University", 
    ufs: "University of the Free State",
    up: "University of Pretoria",
    unisa: "University of South Africa",
    nwu: "North-West University",
    wits: "University of the Witwatersrand",
    cut: "Central University of Technology",
    elis: "E-LIS e-prints in Library and Information Science"
  };
  return map[id] || id.toUpperCase();
}

const pick = (arr) => arr?.[0] || "";
const year = (d) => { 
  if (!d) return undefined;
  // Extract year from date string
  const match = String(d).match(/\b(\d{4})\b/);
  return match ? parseInt(match[1]) : undefined;
};
const stripHtml = (s) => String(s||"").replace(/<[^>]+>/g,"").replace(/\s+/g," ").trim();
function extract(xml, tag) {
  const re = new RegExp(`<${tag}>([\\s\\S]*?)<\\/${tag}>`, "gi");
  const out=[]; 
  let m; 
  while((m=re.exec(xml))) {
    const content = m[1].trim();
    if (content && content.length > 0) {
      const decoded = decode(content);
      if (decoded && decoded.trim()) {
        out.push(decoded.trim());
      }
    }
  }
  return out;
}
function decode(s) { return String(s||"").replace(/&lt;/g,"<").replace(/&gt;/g,">").replace(/&amp;/g,"&").replace(/&quot;/g,'"').replace(/&#39;/g,"'"); }
function rid() { return [...crypto.getRandomValues(new Uint8Array(8))].map(b=>b.toString(16).padStart(2,"0")).join(""); }
function sleep(ms) { return new Promise(resolve => setTimeout(resolve, ms)); }

async function logError(env, context, error) {
  const entry = { context, message: error?.message || String(error), time: new Date().toISOString() };
  try { await env.HARVEST_KV.put(KV.lastErr, JSON.stringify(entry)); }
  catch (e) { console.error("KV log error", e?.message); }
  console.error("âŒ", context, entry.message);
}
